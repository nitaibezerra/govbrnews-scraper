# Plano de MigraÃ§Ã£o: Campo `published_at` (Date â†’ Datetime)

**Status**: ğŸŸ¡ Em Planejamento
**Criado em**: 2025-11-19
**Ãšltima atualizaÃ§Ã£o**: 2025-11-19
**ResponsÃ¡vel**: Nitai Bezerra

---

## ğŸ“‹ Ãndice

1. [Contexto e MotivaÃ§Ã£o](#contexto-e-motivaÃ§Ã£o)
2. [Estado Atual](#estado-atual)
3. [Objetivos da MigraÃ§Ã£o](#objetivos-da-migraÃ§Ã£o)
4. [Arquitetura Atual](#arquitetura-atual)
5. [EstratÃ©gia de MigraÃ§Ã£o](#estratÃ©gia-de-migraÃ§Ã£o)
6. [Plano de ImplementaÃ§Ã£o](#plano-de-implementaÃ§Ã£o)
7. [Scripts e CÃ³digo](#scripts-e-cÃ³digo)
8. [Checklists de ValidaÃ§Ã£o](#checklists-de-validaÃ§Ã£o)
9. [Rollback Plan](#rollback-plan)
10. [Timeline](#timeline)
11. [Riscos e MitigaÃ§Ãµes](#riscos-e-mitigaÃ§Ãµes)

---

## Contexto e MotivaÃ§Ã£o

### O que foi feito no PR #45

O [PR #45](https://github.com/nitaibezerra/govbrnews-scraper/pull/45) introduziu campos de datetime completos no scraper, mantendo retrocompatibilidade:

**Campos adicionados:**
- `published_datetime`: Timestamp completo com hora e timezone (ISO 8601, e.g., "2025-11-17T19:24:43-03:00")
- `updated_datetime`: Timestamp de atualizaÃ§Ã£o quando disponÃ­vel

**MÃ©todos de extraÃ§Ã£o:**
1. **JSON-LD schema** (mais confiÃ¡vel) - extrai de metadados estruturados
2. **PadrÃµes de texto** - detecta formatos como "DD/MM/YYYY HH:MMh"
3. **Timezone padrÃ£o**: BrasÃ­lia (UTC-3)

**Campo mantido:**
- `published_at`: Date-only (sem hora) para retrocompatibilidade

**Bug corrigido:**
- Valores null nÃ£o sÃ£o mais convertidos para epoch Unix (1970-01-01)

### Por que migrar agora?

Com os dados de timestamp disponÃ­veis, podemos:
- âœ… **OrdenaÃ§Ã£o precisa**: NotÃ­cias do mesmo dia ordenadas por hora de publicaÃ§Ã£o
- âœ… **UX melhorado**: Exibir "19h24" no portal ao invÃ©s de sÃ³ a data
- âœ… **SimplificaÃ§Ã£o**: Remover campo duplicado (`published_at` date-only)
- âœ… **Arquitetura limpa**: Um Ãºnico campo datetime como fonte de verdade

---

## Estado Atual

### Estrutura de Dados

**Dataset (HuggingFace: nitaibezerra/govbrnews):**
```python
{
    "published_at": "2024-01-15",              # datetime.date (string no dataset)
    "published_datetime": "2024-01-15T19:24:43-03:00",  # datetime com timezone
    "updated_datetime": "2024-01-16T10:30:00-03:00"     # opcional
}
```

**Typesense Schema:**
```python
{'name': 'published_at', 'type': 'int64', 'facet': False}  # Unix timestamp
```

**Portal (TypeScript):**
```typescript
published_at: number | null  // Unix timestamp
```

### Fluxo de Dados Atual

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. SCRAPER                                                  â”‚
â”‚    Extrai: published_at (date) + published_datetime (dt)   â”‚
â”‚    â†“ Salva no Dataset HuggingFace                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. TYPESENSE LOADING                                        â”‚
â”‚    LÃª: published_at do dataset                             â”‚
â”‚    Converte: datetime.date â†’ Unix timestamp (int64)        â”‚
â”‚    Armazena: published_at como int64 no Typesense          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. PORTAL                                                   â”‚
â”‚    Consulta: published_at como number (timestamp)          â”‚
â”‚    Usa para: OrdenaÃ§Ã£o e filtros                           â”‚
â”‚    Exibe: Somente data (sem hora)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Insight chave**: A conversÃ£o para Unix timestamp acontece no **loading do Typesense**, nÃ£o no scraper. Isso significa que:
- âœ… Typesense aceita qualquer formato datetime
- âœ… NÃ£o precisamos mudar o schema do Typesense
- âœ… Portal nÃ£o precisa mudanÃ§as de tipo

---

## Objetivos da MigraÃ§Ã£o

### Objetivos PrimÃ¡rios

1. âœ… **OrdenaÃ§Ã£o precisa por timestamp** - artigos do mesmo dia ordenados por hora
2. âœ… **Exibir hora no Portal** - mostrar "17/11/2025 Ã s 19h24" quando disponÃ­vel
3. âœ… **Deprecar campo `published_at` (date)** - manter apenas datetime

### Objetivos SecundÃ¡rios

4. âœ… **Simplificar cÃ³digo do scraper** - remover lÃ³gica duplicada
5. âœ… **Zero breaking changes** - nenhum downtime ou mudanÃ§a de schema
6. âœ… **MigraÃ§Ã£o gradual** - implementaÃ§Ã£o por fases com rollback seguro

### NÃ£o-Objetivos

âŒ Mudar schema do Typesense (continua int64)
âŒ Mudar tipo do Portal (continua number)
âŒ Rescrape completo de dados histÃ³ricos
âŒ MudanÃ§as em consumidores externos (nÃ£o existem)

---

## Arquitetura Atual

### Componentes Envolvidos

1. **govbrnews-scraper-main**: ExtraÃ§Ã£o de dados e push para HuggingFace
2. **destaquesgovbr-typesense** (Docker): Loading inicial de dados
3. **destaquesgovbr-infra**: Scripts de loading incremental para produÃ§Ã£o
4. **destaquesgovbr-portal**: Interface web Next.js

### DependÃªncias entre Componentes

```
Scraper (dados) â†’ HuggingFace (storage) â†’ Typesense (indexaÃ§Ã£o) â†’ Portal (UI)
```

**Importante**: Cada componente Ã© independente, permitindo deploy incremental.

---

## EstratÃ©gia de MigraÃ§Ã£o

### Abordagem: RenomeaÃ§Ã£o no Dataset

**Conceito**: Trocar os campos no dataset do HuggingFace sem modificar schemas downstream.

```
ANTES:
published_at (date) â”€â”€â”€â†’ Typesense â”€â”€â”€â†’ Portal
published_datetime (datetime) (nÃ£o usado)

DEPOIS:
published_at (datetime) â”€â”€â”€â†’ Typesense â”€â”€â”€â†’ Portal
published_at_old (date) (temporÃ¡rio, depois removido)
```

### Por que essa abordagem funciona?

1. **Typesense nÃ£o se importa**: Converte date OU datetime â†’ int64
2. **Portal nÃ£o se importa**: Espera number (timestamp), funciona com ambos
3. **Nome mantido**: Campo continua se chamando `published_at` para consumidores
4. **Zero downtime**: MudanÃ§a transparente

### Tratamento de Dados HistÃ³ricos

Artigos antigos (pre-PR #45) nÃ£o tÃªm hora real. EstratÃ©gia:

**OpÃ§Ã£o escolhida: Timestamp sintÃ©tico 00:00:00**
- Artigo de 2024-01-15 â†’ `2024-01-15T00:00:00-03:00`
- **Rationale**: Hora 00:00 indica "sem informaÃ§Ã£o de hora precisa"
- **BenefÃ­cio**: Zero nulls, sorting funciona perfeitamente
- **TransparÃªncia**: Documentado que hora 00:00 = data aproximada

---

## Plano de ImplementaÃ§Ã£o

### VisÃ£o Geral das Fases

| Fase | DescriÃ§Ã£o | DuraÃ§Ã£o | Risco |
|------|-----------|---------|-------|
| 1 | Backfill de dados histÃ³ricos | 1 semana | ğŸŸ¢ Baixo |
| 2 | RenomeaÃ§Ã£o no dataset HuggingFace | 1 dia | ğŸŸ¢ Baixo |
| 3 | AtualizaÃ§Ã£o do scraper | 1 semana | ğŸŸ¢ Baixo |
| 4 | ReindexaÃ§Ã£o e atualizaÃ§Ã£o do portal | 1-2 semanas | ğŸŸ¡ MÃ©dio |
| 5 | Limpeza final | 1 dia | ğŸŸ¢ Baixo |

---

### Fase 1: Backfill de Dados HistÃ³ricos

**Objetivo**: Garantir que todos os artigos tenham `published_datetime` preenchido.

#### Tarefas

1. **Criar script de backfill** (`scripts/backfill_published_datetime.py`)
2. **Processar dataset completo**:
   - Se `published_datetime` existe â†’ manter valor
   - Se nÃ£o existe â†’ criar a partir de `published_at` com hora 00:00:00 (timezone BrasÃ­lia)
3. **Validar resultado**: 0 nulls em `published_datetime`
4. **Push para HuggingFace**

#### Script (ver seÃ§Ã£o Scripts e CÃ³digo)

#### Checklist de ValidaÃ§Ã£o

- [ ] Script criado e testado localmente
- [ ] Dataset baixado do HuggingFace
- [ ] Backfill executado com sucesso
- [ ] ValidaÃ§Ã£o: `df['published_datetime'].isna().sum() == 0`
- [ ] ComparaÃ§Ã£o antes/depois documentada
- [ ] Push para HuggingFace realizado
- [ ] Download de teste confirma dados corretos

#### Riscos e MitigaÃ§Ãµes

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| Dataset corrompido durante processo | Baixa | Alto | Backup do dataset antes de modificar |
| Timestamps invÃ¡lidos | Baixa | MÃ©dio | ValidaÃ§Ã£o com `pd.to_datetime()` antes de salvar |
| Push para HuggingFace falha | Baixa | Baixo | Retry automÃ¡tico, verificaÃ§Ã£o apÃ³s push |

---

### Fase 2: RenomeaÃ§Ã£o no Dataset

**Objetivo**: Trocar os campos `published_at` e `published_datetime`.

#### Tarefas

1. **Baixar dataset atualizado** (pÃ³s-backfill)
2. **Renomear colunas**:
   ```python
   df = df.rename(columns={
       'published_at': 'published_at_old',
       'published_datetime': 'published_at'
   })
   ```
3. **Validar estrutura**:
   - `published_at` agora Ã© datetime
   - `published_at_old` preservado temporariamente
4. **Push para HuggingFace**

#### Script (ver seÃ§Ã£o Scripts e CÃ³digo)

#### Checklist de ValidaÃ§Ã£o

- [ ] Backup do dataset criado
- [ ] RenomeaÃ§Ã£o executada localmente
- [ ] ValidaÃ§Ã£o de tipos: `df['published_at'].dtype == 'datetime64[ns]'`
- [ ] ValidaÃ§Ã£o de valores: comparar `published_at` com `published_at_old`
- [ ] Push para HuggingFace
- [ ] Download de teste confirma estrutura nova

#### Riscos e MitigaÃ§Ãµes

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| Consumidores quebram | Nenhuma* | N/A | *NÃ£o hÃ¡ consumidores externos confirmados |
| Typesense loading quebra | Baixa | MÃ©dio | Testar loading local antes de produÃ§Ã£o |

---

### Fase 3: AtualizaÃ§Ã£o do Scraper

**Objetivo**: Modificar scraper para salvar datetime diretamente como `published_at`.

#### Tarefas

1. **Criar branch**: `feature/published-at-datetime-migration`
2. **Modificar cÃ³digo**:
   - **webscraper.py (linha ~218)**: Renomear `published_datetime` â†’ `published_at`
   - **webscraper.py**: Remover extraÃ§Ã£o de `published_at` (date-only)
   - **scrape_manager.py**: Atualizar referÃªncias
   - **dataset_manager.py**: Atualizar merge/update logic
3. **Atualizar testes**:
   - `test_datetime_scraping.py`: Atualizar assertions
   - `test_new_fields.py`: Remover testes do campo antigo
4. **Testar localmente**:
   - Scrape de teste
   - ValidaÃ§Ã£o de tipos
5. **Criar PR e solicitar review**
6. **Merge apÃ³s aprovaÃ§Ã£o**

#### Arquivos a Modificar

```
src/scraper/webscraper.py (linhas 218-220, 470-656)
src/scraper/scrape_manager.py (linhas 162-165)
src/dataset_manager.py (linhas 159-165, 224-228)
test_datetime_scraping.py
test_new_fields.py
```

#### Checklist de ValidaÃ§Ã£o

- [ ] Branch criada e atualizada com main
- [ ] CÃ³digo modificado
- [ ] Testes atualizados
- [ ] Testes locais passando: `pytest tests/`
- [ ] Scrape de teste realizado
- [ ] Dataset de teste criado e validado
- [ ] PR criado com descriÃ§Ã£o detalhada
- [ ] Review aprovado
- [ ] Merge realizado
- [ ] CI/CD passa apÃ³s merge

#### Riscos e MitigaÃ§Ãµes

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| Quebrar scraping ativo | Baixa | Alto | Testes extensivos antes de merge |
| RegressÃ£o em extraÃ§Ã£o de datetime | MÃ©dia | MÃ©dio | Manter testes do PR #45 intactos |
| Merge conflicts | Baixa | Baixo | Atualizar branch regularmente |

---

### Fase 4: ReindexaÃ§Ã£o e AtualizaÃ§Ã£o do Portal

**Objetivo**: Reindexar Typesense com novos dados e atualizar Portal para exibir hora.

#### Parte 4A: ReindexaÃ§Ã£o do Typesense

##### Tarefas

1. **Atualizar Typesense Docker** (se necessÃ¡rio)
   - Verificar se loading script jÃ¡ suporta datetime
   - Testar localmente: `./run-typesense-server.sh cleanup && ./run-typesense-server.sh`
2. **Atualizar scripts de infra**
   - Mesmo cÃ³digo de loading
   - Testar em staging se disponÃ­vel
3. **ReindexaÃ§Ã£o em produÃ§Ã£o**:
   - OpÃ§Ã£o A: Full reindex (deleta + recria collection)
   - OpÃ§Ã£o B: Incremental load de todo o dataset
4. **ValidaÃ§Ã£o**: Queries de teste no Typesense

##### Checklist de ValidaÃ§Ã£o

- [ ] Loading script do Docker testado localmente
- [ ] Dados carregados corretamente (timestamps vÃ¡lidos)
- [ ] Scripts de infra atualizados
- [ ] DecisÃ£o tomada: full vs incremental reindex
- [ ] Backup de produÃ§Ã£o realizado
- [ ] Reindex executado
- [ ] ValidaÃ§Ã£o: contagem de documentos
- [ ] ValidaÃ§Ã£o: sorting por `published_at` funciona
- [ ] ValidaÃ§Ã£o: filtros por data funcionam

#### Parte 4B: AtualizaÃ§Ã£o do Portal

##### Tarefas

1. **Criar branch**: `feature/show-publication-time`
2. **Atualizar componentes de exibiÃ§Ã£o**:
   - Identificar componentes que mostram `published_at`
   - Implementar formataÃ§Ã£o condicional:
     - Se hora != 00:00 â†’ "17/11/2025 Ã s 19h24"
     - Se hora == 00:00 â†’ "17/11/2025" (sem hora)
3. **Atualizar tipos TypeScript** (se necessÃ¡rio)
4. **Testes**:
   - Testes unitÃ¡rios de componentes
   - Testes de integraÃ§Ã£o
   - Testes visuais/screenshots
5. **Criar PR e review**
6. **Deploy**

##### Arquivos a Modificar (aproximados)

```
src/lib/article-row.ts (tipos)
src/app/actions.ts (queries - nenhuma mudanÃ§a necessÃ¡ria)
src/components/<ArticleCard|ArticleList>.tsx (formataÃ§Ã£o de data)
```

##### Checklist de ValidaÃ§Ã£o

- [ ] Branch criada
- [ ] Componentes identificados
- [ ] CÃ³digo de formataÃ§Ã£o implementado
- [ ] Testes unitÃ¡rios passando
- [ ] Testes de integraÃ§Ã£o passando
- [ ] Preview visual validado
- [ ] PR criado e revisado
- [ ] Deploy em staging (se disponÃ­vel)
- [ ] ValidaÃ§Ã£o em staging
- [ ] Deploy em produÃ§Ã£o
- [ ] Smoke tests em produÃ§Ã£o

#### Riscos e MitigaÃ§Ãµes

| Risco | Probabilidade | Impacto | MitigaÃ§Ã£o |
|-------|---------------|---------|-----------|
| Downtime durante reindex | Baixa | MÃ©dio | Reindex incremental em vez de drop + recreate |
| Portal mostra timestamps incorretos | MÃ©dia | Alto | Testes extensivos de formataÃ§Ã£o, considerar timezone |
| Performance de queries degradada | Baixa | MÃ©dio | Monitorar mÃ©tricas apÃ³s deploy |

---

### Fase 5: Limpeza Final

**Objetivo**: Remover campo temporÃ¡rio `published_at_old`.

#### Tarefas

1. **Validar tudo funcionando**:
   - Scraper salvando novos dados
   - Typesense indexando corretamente
   - Portal exibindo hora
2. **Remover coluna do dataset**:
   ```python
   df = df.drop(columns=['published_at_old'])
   ```
3. **Push para HuggingFace**
4. **Documentar migraÃ§Ã£o**:
   - Atualizar README se necessÃ¡rio
   - Adicionar entry no CHANGELOG
   - Marcar este documento como concluÃ­do

#### Checklist de ValidaÃ§Ã£o

- [ ] Sistema rodando estÃ¡vel por 1-2 semanas
- [ ] Nenhum erro relacionado a `published_at` nos logs
- [ ] ConfirmaÃ§Ã£o: nenhum cÃ³digo referencia `published_at_old`
- [ ] Coluna removida do dataset
- [ ] Push para HuggingFace
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] MigraÃ§Ã£o marcada como âœ… ConcluÃ­da

---

## Scripts e CÃ³digo

### Script 1: Backfill `published_datetime`

**Arquivo**: `scripts/backfill_published_datetime.py`

```python
"""
Script para backfill do campo published_datetime.

Para artigos que nÃ£o possuem published_datetime, cria um timestamp
a partir de published_at com hora 00:00:00 (timezone BrasÃ­lia).

Uso:
    python scripts/backfill_published_datetime.py
"""

import pandas as pd
from datetime import datetime
from datasets import load_dataset, Dataset
from zoneinfo import ZoneInfo

# ConfiguraÃ§Ãµes
DATASET_NAME = "nitaibezerra/govbrnews"
BRASILIA_TZ = ZoneInfo("America/Sao_Paulo")  # UTC-3


def backfill_published_datetime(df: pd.DataFrame) -> pd.DataFrame:
    """
    Preenche published_datetime quando ausente.

    Args:
        df: DataFrame com colunas published_at e published_datetime

    Returns:
        DataFrame com published_datetime preenchido
    """
    print(f"Total de artigos: {len(df)}")

    # Contar quantos jÃ¡ tÃªm published_datetime
    has_datetime = df['published_datetime'].notna().sum()
    print(f"Artigos com published_datetime: {has_datetime}")
    print(f"Artigos sem published_datetime: {len(df) - has_datetime}")

    # Para cada artigo sem published_datetime
    def fill_datetime(row):
        if pd.notna(row['published_datetime']):
            # JÃ¡ tem datetime, manter valor
            return row['published_datetime']

        if pd.isna(row['published_at']):
            # Sem nenhuma data, manter None
            return None

        # Criar datetime a partir de published_at com hora 00:00:00
        date = pd.to_datetime(row['published_at']).date()
        dt = datetime.combine(date, datetime.min.time())
        dt = dt.replace(tzinfo=BRASILIA_TZ)

        return dt.isoformat()

    df['published_datetime'] = df.apply(fill_datetime, axis=1)

    # ValidaÃ§Ã£o
    nulls_after = df['published_datetime'].isna().sum()
    print(f"\nApÃ³s backfill:")
    print(f"  Nulls em published_datetime: {nulls_after}")
    print(f"  Artigos preenchidos: {len(df) - nulls_after}")

    return df


def main():
    """Executa o backfill completo."""
    print("=" * 60)
    print("BACKFILL DE PUBLISHED_DATETIME")
    print("=" * 60)

    # 1. Carregar dataset
    print("\n1. Carregando dataset do HuggingFace...")
    dataset = load_dataset(DATASET_NAME, split='train')
    df = dataset.to_pandas()

    # 2. Fazer backup
    print("\n2. Criando backup local...")
    backup_file = f"backup_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
    df.to_parquet(backup_file)
    print(f"   Backup salvo em: {backup_file}")

    # 3. Executar backfill
    print("\n3. Executando backfill...")
    df_updated = backfill_published_datetime(df)

    # 4. Validar resultado
    print("\n4. Validando resultado...")
    assert df_updated['published_datetime'].isna().sum() == 0, \
        "ERRO: Ainda existem nulls em published_datetime!"

    # Verificar formato de datetime
    sample_datetime = df_updated[df_updated['published_datetime'].notna()]['published_datetime'].iloc[0]
    print(f"   Exemplo de datetime: {sample_datetime}")

    # 5. Salvar localmente
    print("\n5. Salvando dataset atualizado...")
    output_file = "dataset_with_backfilled_datetime.parquet"
    df_updated.to_parquet(output_file)
    print(f"   Salvo em: {output_file}")

    # 6. Fazer push para HuggingFace
    print("\n6. Fazendo push para HuggingFace...")
    response = input("   Deseja fazer push? (sim/nÃ£o): ")

    if response.lower() in ['sim', 's', 'yes', 'y']:
        updated_dataset = Dataset.from_pandas(df_updated)
        updated_dataset.push_to_hub(DATASET_NAME, private=False)
        print("   âœ… Push realizado com sucesso!")
    else:
        print("   â­ï¸  Push cancelado. Execute manualmente quando pronto.")

    print("\n" + "=" * 60)
    print("BACKFILL CONCLUÃDO COM SUCESSO!")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

---

### Script 2: Renomear Campos no Dataset

**Arquivo**: `scripts/rename_published_at_fields.py`

```python
"""
Script para renomear campos published_at no dataset.

Renomeia:
- published_at â†’ published_at_old (backup)
- published_datetime â†’ published_at (campo principal)

Uso:
    python scripts/rename_published_at_fields.py
"""

import pandas as pd
from datetime import datetime
from datasets import load_dataset, Dataset

# ConfiguraÃ§Ãµes
DATASET_NAME = "nitaibezerra/govbrnews"


def rename_fields(df: pd.DataFrame) -> pd.DataFrame:
    """
    Renomeia campos do dataset.

    Args:
        df: DataFrame original

    Returns:
        DataFrame com campos renomeados
    """
    print(f"Colunas antes: {list(df.columns)}")

    # Validar que campos existem
    required_cols = ['published_at', 'published_datetime']
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        raise ValueError(f"Colunas faltando no dataset: {missing}")

    # Renomear
    df_renamed = df.rename(columns={
        'published_at': 'published_at_old',
        'published_datetime': 'published_at'
    })

    print(f"Colunas depois: {list(df_renamed.columns)}")

    # Validar tipos
    print(f"\nTipo de 'published_at' (novo): {df_renamed['published_at'].dtype}")
    print(f"Tipo de 'published_at_old': {df_renamed['published_at_old'].dtype}")

    # Comparar valores (sanity check)
    print("\nExemplos de dados:")
    print(df_renamed[['published_at', 'published_at_old']].head())

    return df_renamed


def main():
    """Executa a renomeaÃ§Ã£o."""
    print("=" * 60)
    print("RENOMEAÃ‡ÃƒO DE CAMPOS PUBLISHED_AT")
    print("=" * 60)

    # 1. Carregar dataset
    print("\n1. Carregando dataset do HuggingFace...")
    dataset = load_dataset(DATASET_NAME, split='train')
    df = dataset.to_pandas()

    # 2. Fazer backup
    print("\n2. Criando backup local...")
    backup_file = f"backup_before_rename_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
    df.to_parquet(backup_file)
    print(f"   Backup salvo em: {backup_file}")

    # 3. Renomear campos
    print("\n3. Renomeando campos...")
    df_renamed = rename_fields(df)

    # 4. Validar resultado
    print("\n4. Validando resultado...")
    assert 'published_at' in df_renamed.columns, "Campo 'published_at' nÃ£o existe!"
    assert 'published_at_old' in df_renamed.columns, "Campo 'published_at_old' nÃ£o existe!"
    assert 'published_datetime' not in df_renamed.columns, "Campo 'published_datetime' ainda existe!"

    # Verificar nulls
    nulls = df_renamed['published_at'].isna().sum()
    print(f"   Nulls em published_at (novo): {nulls}")

    # 5. Salvar localmente
    print("\n5. Salvando dataset renomeado...")
    output_file = "dataset_renamed.parquet"
    df_renamed.to_parquet(output_file)
    print(f"   Salvo em: {output_file}")

    # 6. Fazer push para HuggingFace
    print("\n6. Fazendo push para HuggingFace...")
    response = input("   Deseja fazer push? (sim/nÃ£o): ")

    if response.lower() in ['sim', 's', 'yes', 'y']:
        renamed_dataset = Dataset.from_pandas(df_renamed)
        renamed_dataset.push_to_hub(DATASET_NAME, private=False)
        print("   âœ… Push realizado com sucesso!")
    else:
        print("   â­ï¸  Push cancelado. Execute manualmente quando pronto.")

    print("\n" + "=" * 60)
    print("RENOMEAÃ‡ÃƒO CONCLUÃDA COM SUCESSO!")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

---

### Script 3: Remover Campo TemporÃ¡rio

**Arquivo**: `scripts/remove_published_at_old.py`

```python
"""
Script para remover campo temporÃ¡rio published_at_old.

Uso:
    python scripts/remove_published_at_old.py
"""

import pandas as pd
from datetime import datetime
from datasets import load_dataset, Dataset

# ConfiguraÃ§Ãµes
DATASET_NAME = "nitaibezerra/govbrnews"


def main():
    """Remove coluna published_at_old."""
    print("=" * 60)
    print("REMOÃ‡ÃƒO DE CAMPO PUBLISHED_AT_OLD")
    print("=" * 60)

    # 1. Carregar dataset
    print("\n1. Carregando dataset do HuggingFace...")
    dataset = load_dataset(DATASET_NAME, split='train')
    df = dataset.to_pandas()

    # 2. Verificar se coluna existe
    if 'published_at_old' not in df.columns:
        print("   âš ï¸  Coluna 'published_at_old' nÃ£o encontrada. Nada a fazer.")
        return

    print(f"   Colunas atuais: {list(df.columns)}")

    # 3. Fazer backup
    print("\n2. Criando backup local...")
    backup_file = f"backup_before_cleanup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
    df.to_parquet(backup_file)
    print(f"   Backup salvo em: {backup_file}")

    # 4. Remover coluna
    print("\n3. Removendo coluna 'published_at_old'...")
    df_cleaned = df.drop(columns=['published_at_old'])
    print(f"   Colunas apÃ³s remoÃ§Ã£o: {list(df_cleaned.columns)}")

    # 5. Salvar localmente
    print("\n4. Salvando dataset limpo...")
    output_file = "dataset_cleaned.parquet"
    df_cleaned.to_parquet(output_file)
    print(f"   Salvo em: {output_file}")

    # 6. Fazer push para HuggingFace
    print("\n5. Fazendo push para HuggingFace...")
    response = input("   Deseja fazer push? (sim/nÃ£o): ")

    if response.lower() in ['sim', 's', 'yes', 'y']:
        cleaned_dataset = Dataset.from_pandas(df_cleaned)
        cleaned_dataset.push_to_hub(DATASET_NAME, private=False)
        print("   âœ… Push realizado com sucesso!")
    else:
        print("   â­ï¸  Push cancelado. Execute manualmente quando pronto.")

    print("\n" + "=" * 60)
    print("LIMPEZA CONCLUÃDA COM SUCESSO!")
    print("=" * 60)


if __name__ == "__main__":
    main()
```

---

## Checklists de ValidaÃ§Ã£o

### Checklist Geral da MigraÃ§Ã£o

#### PrÃ©-MigraÃ§Ã£o
- [ ] Documento de migraÃ§Ã£o revisado e aprovado
- [ ] Backups de todos os componentes criados:
  - [ ] Dataset HuggingFace
  - [ ] Collection Typesense (se aplicÃ¡vel)
  - [ ] CÃ³digo dos repositÃ³rios (tags Git)
- [ ] ComunicaÃ§Ã£o com stakeholders realizada
- [ ] Janela de manutenÃ§Ã£o agendada (se necessÃ¡rio)

#### Durante MigraÃ§Ã£o
- [ ] Fase 1 (Backfill) concluÃ­da e validada
- [ ] Fase 2 (RenomeaÃ§Ã£o) concluÃ­da e validada
- [ ] Fase 3 (Scraper) concluÃ­da e validada
- [ ] Fase 4 (Typesense + Portal) concluÃ­da e validada
- [ ] Fase 5 (Limpeza) concluÃ­da e validada

#### PÃ³s-MigraÃ§Ã£o
- [ ] Sistema em produÃ§Ã£o estÃ¡vel por 1+ semana
- [ ] Nenhum erro relacionado Ã  migraÃ§Ã£o nos logs
- [ ] MÃ©tricas de performance normais
- [ ] Testes end-to-end passando
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Post-mortem realizado (se houve incidentes)

---

### Checklist de Testes

#### Testes de Scraper
- [ ] Scraping de URLs de teste
- [ ] ValidaÃ§Ã£o de extraÃ§Ã£o de `published_at` (datetime)
- [ ] ValidaÃ§Ã£o de timezone (BrasÃ­lia)
- [ ] Testes unitÃ¡rios passando
- [ ] Testes de integraÃ§Ã£o passando

#### Testes de Typesense
- [ ] Loading local de dataset de teste
- [ ] ValidaÃ§Ã£o de conversÃ£o datetime â†’ timestamp
- [ ] Queries de ordenaÃ§Ã£o por `published_at`
- [ ] Queries de filtro por data
- [ ] Performance de queries

#### Testes de Portal
- [ ] ExibiÃ§Ã£o correta de data + hora
- [ ] ExibiÃ§Ã£o correta de apenas data (para hora 00:00)
- [ ] OrdenaÃ§Ã£o de artigos
- [ ] Filtros por data
- [ ] Testes de responsividade
- [ ] Testes de acessibilidade

---

## Rollback Plan

### CenÃ¡rios de Rollback

#### Fase 1 - Rollback do Backfill

**Quando**: Backfill gerou dados incorretos.

**AÃ§Ãµes**:
1. Restaurar dataset do backup: `backup_dataset_<timestamp>.parquet`
2. Fazer push do backup para HuggingFace
3. Validar restauraÃ§Ã£o

**Tempo estimado**: 30 minutos

---

#### Fase 2 - Rollback da RenomeaÃ§Ã£o

**Quando**: RenomeaÃ§Ã£o quebrou consumidores ou gerou dados incorretos.

**AÃ§Ãµes**:
1. Restaurar dataset do backup: `backup_before_rename_<timestamp>.parquet`
2. Fazer push do backup para HuggingFace
3. Validar restauraÃ§Ã£o

**Tempo estimado**: 30 minutos

---

#### Fase 3 - Rollback do Scraper

**Quando**: Scraper nÃ£o estÃ¡ funcionando corretamente apÃ³s mudanÃ§as.

**AÃ§Ãµes**:
1. Reverter commit: `git revert <commit-hash>`
2. Fazer push da reversÃ£o
3. CI/CD deploy automÃ¡tico
4. Validar scraping funcionando

**Tempo estimado**: 15 minutos

---

#### Fase 4 - Rollback de Typesense

**Quando**: Reindex causou problemas ou Portal nÃ£o funciona.

**AÃ§Ãµes**:

**OpÃ§Ã£o A - Reindexar com dados antigos**:
1. Restaurar dataset do backup
2. Reindexar Typesense
3. Validar

**OpÃ§Ã£o B - Restaurar backup de collection** (se disponÃ­vel):
1. Deletar collection atual
2. Restaurar snapshot
3. Validar

**Tempo estimado**: 30-60 minutos (dependendo do tamanho do dataset)

---

#### Fase 4 - Rollback do Portal

**Quando**: Portal apresenta bugs ou exibiÃ§Ã£o incorreta.

**AÃ§Ãµes**:
1. Reverter deploy para versÃ£o anterior
2. Validar funcionamento
3. Investigar causa raiz

**Tempo estimado**: 10 minutos (deploy automÃ¡tico)

---

#### Fase 5 - Rollback da Limpeza

**Quando**: Descoberta de dependÃªncia em `published_at_old`.

**AÃ§Ãµes**:
1. Restaurar dataset do backup: `backup_before_cleanup_<timestamp>.parquet`
2. Fazer push do backup para HuggingFace
3. Validar

**Tempo estimado**: 30 minutos

---

### Plano de ComunicaÃ§Ã£o em Caso de Rollback

1. **Notificar stakeholders**: Email/Slack com motivo do rollback
2. **Atualizar status da migraÃ§Ã£o**: Marcar fase como "falhou"
3. **InvestigaÃ§Ã£o**: Post-mortem para entender causa raiz
4. **DecisÃ£o**: Corrigir e tentar novamente, ou abortar migraÃ§Ã£o

---

## Timeline

### Timeline Otimista (Sem Problemas)

| Semana | Atividades | Status |
|--------|-----------|--------|
| **Semana 1** | Planejamento e aprovaÃ§Ã£o | ğŸŸ¡ Em andamento |
| **Semana 2** | Fase 1: Backfill de dados histÃ³ricos | ğŸ”µ Planejado |
| **Semana 3** | Fase 2: RenomeaÃ§Ã£o no dataset | ğŸ”µ Planejado |
| **Semana 4-5** | Fase 3: AtualizaÃ§Ã£o do scraper | ğŸ”µ Planejado |
| **Semana 6-7** | Fase 4: Typesense + Portal | ğŸ”µ Planejado |
| **Semana 8** | Fase 5: Limpeza e documentaÃ§Ã£o | ğŸ”µ Planejado |
| **Semana 9-10** | Monitoramento pÃ³s-migraÃ§Ã£o | ğŸ”µ Planejado |

**Total**: ~10 semanas (2,5 meses)

---

### Timeline Realista (Com ContingÃªncias)

| Semana | Atividades | Buffer |
|--------|-----------|--------|
| **Semana 1-2** | Planejamento e aprovaÃ§Ã£o | +1 semana para discussÃµes |
| **Semana 3-4** | Fase 1: Backfill | +1 semana para problemas de dados |
| **Semana 5** | Fase 2: RenomeaÃ§Ã£o | +1 semana para validaÃ§Ã£o extra |
| **Semana 6-8** | Fase 3: Scraper | +1 semana para bugs/testes |
| **Semana 9-12** | Fase 4: Typesense + Portal | +2 semanas para ajustes de UI |
| **Semana 13** | Fase 5: Limpeza | - |
| **Semana 14-16** | Monitoramento pÃ³s-migraÃ§Ã£o | +2 semanas para estabilizaÃ§Ã£o |

**Total**: ~16 semanas (4 meses)

---

### Milestones CrÃ­ticos

| Milestone | Data Alvo | CritÃ©rio de Sucesso |
|-----------|-----------|---------------------|
| **M1**: Plano aprovado | Semana 2 | Documento assinado por stakeholders |
| **M2**: Backfill concluÃ­do | Semana 4 | Dataset sem nulls em `published_datetime` |
| **M3**: Scraper atualizado | Semana 8 | PR merged e CI verde |
| **M4**: Portal mostrando hora | Semana 12 | Deploy em produÃ§Ã£o funcionando |
| **M5**: MigraÃ§Ã£o concluÃ­da | Semana 16 | Sistema estÃ¡vel, documentaÃ§Ã£o finalizada |

---

## Riscos e MitigaÃ§Ãµes

### Matriz de Riscos

| ID | Risco | Prob. | Impacto | Severidade | MitigaÃ§Ã£o |
|----|-------|-------|---------|------------|-----------|
| R1 | Dados histÃ³ricos corrompidos durante backfill | Baixa | Alto | ğŸŸ¡ MÃ©dio | Backup antes de modificar, validaÃ§Ã£o extensiva |
| R2 | Typesense loading quebra com novo formato | Baixa | MÃ©dio | ğŸŸ¢ Baixo | Testar localmente antes de produÃ§Ã£o |
| R3 | Portal mostra timestamps incorretos | MÃ©dia | Alto | ğŸ”´ Alto | Testes extensivos de timezone, validaÃ§Ã£o visual |
| R4 | Performance de queries degradada | Baixa | MÃ©dio | ğŸŸ¡ MÃ©dio | Monitoramento, Ã­ndices otimizados |
| R5 | Scraper para de funcionar apÃ³s mudanÃ§as | Baixa | Alto | ğŸŸ¡ MÃ©dio | Testes automatizados, rollback rÃ¡pido |
| R6 | Downtime prolongado durante reindex | Baixa | MÃ©dio | ğŸŸ¢ Baixo | Usar incremental load em vez de full reindex |
| R7 | Descoberta de consumidores externos nÃ£o documentados | Muito Baixa | MÃ©dio | ğŸŸ¢ Baixo | Manter campo temporÃ¡rio `published_at_old` |
| R8 | Problemas com timezone (UTC vs BrasÃ­lia) | MÃ©dia | Alto | ğŸ”´ Alto | ValidaÃ§Ã£o rigorosa, testes com diferentes timezones |

---

### Detalhamento de Riscos CrÃ­ticos

#### R3: Portal mostra timestamps incorretos

**CenÃ¡rio**: Portal exibe hora errada devido a conversÃµes de timezone.

**Sinais de alerta**:
- UsuÃ¡rios reportam horÃ¡rios errados
- Artigos ordenados incorretamente
- Timestamps com diferenÃ§a de 3h (UTC vs BrasÃ­lia)

**MitigaÃ§Ã£o**:
1. **Antes do deploy**: Testes manuais extensivos com diferentes horÃ¡rios
2. **Durante o deploy**: Deploy em staging primeiro
3. **ApÃ³s o deploy**: Monitoramento de logs, feedback de usuÃ¡rios

**Rollback**: Reverter deploy do portal (rÃ¡pido, ~10 min)

---

#### R8: Problemas com timezone

**CenÃ¡rio**: InconsistÃªncia entre timezone do scraper (BrasÃ­lia) e exibiÃ§Ã£o no portal.

**Exemplo problemÃ¡tico**:
- Artigo publicado 17/11/2025 19:24 (BrasÃ­lia)
- Armazenado como Unix timestamp: 1731878640
- Portal exibe: 17/11/2025 22:24 (se interpretar como UTC)

**MitigaÃ§Ã£o**:
1. **Scraper**: Sempre usar `ZoneInfo("America/Sao_Paulo")` ao criar datetime
2. **Typesense**: Armazenar Unix timestamp (UTC universal)
3. **Portal**: Converter de UTC para BrasÃ­lia na exibiÃ§Ã£o:
   ```typescript
   const date = new Date(timestamp * 1000);
   const brasiliaTime = date.toLocaleString('pt-BR', {
     timeZone: 'America/Sao_Paulo'
   });
   ```

**ValidaÃ§Ã£o**:
- Testar com artigos de diferentes horÃ¡rios
- Verificar que exibiÃ§Ã£o corresponde ao esperado
- Comparar com timestamp Unix calculado manualmente

---

## Recursos Adicionais

### DocumentaÃ§Ã£o de ReferÃªncia

- [PR #45 - Datetime Fields](https://github.com/nitaibezerra/govbrnews-scraper/pull/45)
- [Typesense Timestamps](https://typesense.org/docs/latest/api/documents.html#indexing-dates)
- [Pandas Datetime](https://pandas.pydata.org/docs/user_guide/timeseries.html)
- [Python ZoneInfo](https://docs.python.org/3/library/zoneinfo.html)
- [HuggingFace Datasets](https://huggingface.co/docs/datasets/)

### Comandos Ãšteis

```bash
# Scraper
cd /Users/nitai/Dropbox/dev-mgi/govbrnews-scraper-main
python scripts/backfill_published_datetime.py
python scripts/rename_published_at_fields.py
pytest tests/

# Typesense Docker
cd /Users/nitai/Dropbox/dev-mgi/destaquesgovbr-typesense
./run-typesense-server.sh cleanup
./run-typesense-server.sh

# Infra
cd /Users/nitai/Dropbox/dev-mgi/destaquesgovbr-infra/scripts/typesense/python
python scripts/load_data.py --mode full --force
python scripts/load_data.py --mode incremental --days 30

# Portal
cd /Users/nitai/Dropbox/dev-mgi/destaquesgovbr-portal
npm run dev
npm run build
npm run test
```

### Contatos

- **ResponsÃ¡vel pela migraÃ§Ã£o**: Nitai Bezerra
- **RepositÃ³rios**:
  - Scraper: https://github.com/nitaibezerra/govbrnews-scraper
  - Portal: (URL se disponÃ­vel)
  - Infra: (URL se disponÃ­vel)

---

## Controle de VersÃ£o

| VersÃ£o | Data | Autor | MudanÃ§as |
|--------|------|-------|----------|
| 1.0 | 2025-11-19 | Claude Code | VersÃ£o inicial do plano |

---

## Status Atual da MigraÃ§Ã£o

**Status**: ğŸŸ¡ Em Planejamento

### Progresso por Fase

- [ ] **Fase 1**: Backfill de dados histÃ³ricos
- [ ] **Fase 2**: RenomeaÃ§Ã£o no dataset
- [ ] **Fase 3**: AtualizaÃ§Ã£o do scraper
- [ ] **Fase 4**: ReindexaÃ§Ã£o e atualizaÃ§Ã£o do portal
- [ ] **Fase 5**: Limpeza final

**Ãšltima atualizaÃ§Ã£o**: 2025-11-19
**PrÃ³ximo passo**: Criar scripts de backfill e testar localmente

---

## AprovaÃ§Ãµes

| Stakeholder | Papel | Status | Data |
|-------------|-------|--------|------|
| Nitai Bezerra | Tech Lead | ğŸŸ¡ Pendente | - |

---

## Notas Finais

Este plano foi elaborado para garantir uma migraÃ§Ã£o segura, gradual e sem downtime do campo `published_at` de date para datetime. A abordagem de renomeaÃ§Ã£o no dataset foi escolhida por:

âœ… **Simplicidade**: Menos mudanÃ§as de cÃ³digo
âœ… **SeguranÃ§a**: Rollback fÃ¡cil em cada fase
âœ… **Zero downtime**: MudanÃ§as transparentes para usuÃ¡rios
âœ… **Backward compatible**: Nenhum breaking change

O sucesso da migraÃ§Ã£o depende de:
1. ExecuÃ§Ã£o cuidadosa de cada fase
2. ValidaÃ§Ã£o rigorosa apÃ³s cada etapa
3. Monitoramento contÃ­nuo
4. ComunicaÃ§Ã£o clara com stakeholders

**Boa sorte com a migraÃ§Ã£o! ğŸš€**
